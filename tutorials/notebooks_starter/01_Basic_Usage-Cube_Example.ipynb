{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7A6j7pHKEWgC"
   },
   "source": [
    "# Getting started: Basic Cube\n",
    "\n",
    "**Welcome**\n",
    "\n",
    "Welcome to your first EncoderMap tutorial. All EncoderMap tutorials are provided as jupyter notebooks, that you can run locally, on binderhub, or even on google colab.\n",
    "\n",
    "Run this notebook on Google Colab:\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/AG-Peter/encodermap/blob/main/tutorials/notebooks_starter/01_Basic_Usage-Cube_Example.ipynb)\n",
    "\n",
    "Find the documentation of EncoderMap:\n",
    "\n",
    "https://ag-peter.github.io/encodermap\n",
    "\n",
    "**Goals:**\n",
    "\n",
    "In this tutorial you will learn:\n",
    "- [How to set training parameters for EncoderMap.](#select-parameters)\n",
    "- [How to train EncoderMap.](#perform-dimensionality-reduction)\n",
    "- [How to use the decoder part of the network to create high-dimensional data.](#generate-high-dimensional-data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "CNvfnyZyEWgD",
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "**For Google colab only:**\n",
    "\n",
    "If you're on Google colab, please uncomment these lines and install EncoderMap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://raw.githubusercontent.com/AG-Peter/encodermap/main/tutorials/install_encodermap_google_colab.sh\n",
    "# !sudo bash install_encodermap_google_colab.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "ek5-hP-WEWgF",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Import Libraries\n",
    "Before we can get started using EncoderMap we first need to import the EncoderMap library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import encodermap as em"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "QE_obd3YEWgF",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "We will also need some aditional imports for plotting. The line with `google.colab` imports some nice features for google colab, which renders pandas Dataframes very nicely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly\n",
    "import plotly.graph_objs as go\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "try:\n",
    "    from google.colab import data_table, output\n",
    "    data_table.enable_dataframe_formatter()\n",
    "    output.enable_custom_widget_manager()\n",
    "    renderer = \"colab\"\n",
    "except ModuleNotFoundError:\n",
    "    renderer = \"plotly_mimetype+notebook\"\n",
    "pio.renderers.default = renderer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "HMWIMDMZEWgG",
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "To ensure that this notebook yields reproducible output, we fix the randomness in tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.random.set_seed(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "9k3lArfCEWgG",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Load Data\n",
    "Next, we need to load our data. EncoderMap expects the input data to be a 2d array. Each line should contain one data point and the number of columns is the dimensionality of the data set. Here, you could load data from any source. In this tutorial, however, we will use a function to generate a toy data set. The function `random_on_cube_edges` distributes a given number of points randomly on the edges of a cube. We can also add some Gaussian noise by specifying a sigma value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_d_data, ids = em.misc.create_n_cube()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "vkyraGl3EWgG",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "let's look at the data we have just created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    np.vstack([ids, high_d_data.T]).T,\n",
    "    columns=[\"id\", \"x\", \"y\", \"z\"],\n",
    "    index=[f\"Point {i}\" for i in range(len(high_d_data))]\n",
    ").astype({\"id\": \"int\"})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "Vj6Bn8zMgBDl",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "We can now plot the data like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "nbsphinx-thumbnail"
    ]
   },
   "outputs": [],
   "source": [
    "fig = px.scatter_3d(df, x='x', y='y', z='z', color='id', color_continuous_scale  = plotly.colors.sequential.Viridis)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "NG6AJ77GEWgG",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "As you can see, we have a fuzzy cube. The edges of the cube are described by some points in 3d space. The colors of the points correspond to the `id` column of our dataframe. Note, how some colors appear on two edges. Try to keep track of these special edges throughout this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "geIGGCbpEWgH",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<a id='select-parameters'></a>\n",
    "\n",
    "## Select Parameters\n",
    "Now  that we have loaded our data we need to select parameters for EncoderMap. Parameters are stored in an instance of the `Parameters` class. A list of the available parameters can be found [here](https://ag-peter.github.io/encodermap/reference/parameter_classes.html#parameters). Most of the default parameters are fine for our example. Some parameters will need adjustment. These are:\n",
    "\n",
    "- periodicity\n",
    "  - This parameter defines the periodicity of the space of your input data. This is important if your data consists of angles, in which case the default periodicity of pi is good. In our case, the data lies in a non-periodic euclidean space and we set the periodicity to `float(\"inf\")`.\n",
    "- n_steps\n",
    "  - This is the number of training steps. For our small example 200 steps is enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = em.Parameters(\n",
    "periodicity = float(\"inf\"),\n",
    "n_steps = 200,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "U-6AOeTuEWgH",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Furthermore, we should adjust the sigmoid functions applied to the high-dimensional and low-dimensional pairwise distances of the distance based part of the cost function. There a three parameters for each sigmoid which should be given in the following order:  \n",
    "`(sig_h, a_h, b_h, sig_l, a_l, b_l)`  \n",
    "In order to select these parameters it is helpful to plot the sigmoid functions together with a histogram of the pairwise distances in the data set. In the next cell, you can experiment with these parameters. If you don't feel like playing around, the `initial_guess` parameter is a good guess for this system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em.plot.distance_histogram_interactive(\n",
    "    high_d_data,\n",
    "    parameters.periodicity,\n",
    "    bins=50,\n",
    "    initial_guess=(0.3, 6, 6, 1, 4, 6),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "ihGAK7kJEWgH",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "The upper plot shows a histogram of the pairwise distances. The pairwise distances between $n$ points in a space of any dimension can be represented as matrix $D$:\n",
    "\n",
    "\\begin{equation}\n",
    "D = \\begin{bmatrix}\n",
    "d_{11} & d_{12} & \\dots & d_{1n} \\\\\n",
    "d_{21} & d_{22} & \\dots & d_{2n} \\\\\n",
    "\\vdots & & \\ddots & \\vdots \\\\\n",
    "d_{n1} & d_{n2} & \\dots & d_{nn}\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    ", where $d_{ij}$, the distance between point $r_i$ and $r_j$ can be given as:\n",
    "\n",
    "\\begin{equation}\n",
    "d_{ij} = \\lVert r_j - r_j \\rVert\n",
    "\\end{equation}\n",
    "\n",
    "or:\n",
    "\n",
    "\\begin{equation}\n",
    "d_{ij} = \\begin{cases}\n",
    "\\lVert r_j - r_j \\rVert, \\text{if $d_{ij} <=  p$} \\\\\n",
    "\\lVert r_j - r_j \\rVert - p, \\text{if $d_{ij} >  p$}\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "for periodic systems obeying the minimum image convention in a system with space with a periodicity $p$. In the same plot, the high-d sigmoid function and its derivative is shown. This derivative shows the sensitive range of the distance based part of the cost function. As it is not possible to preserve all pairwise distances in the low-d representation we want to tune this sensitive range to match distances which are most important for us. Usually very short distances are not important for the structure of a data set as these distances stem from points inside the same local region. Long distances might be interesting but can hardly be reproduced in a lower dimensional representation. Somewhere in between are the most important distances which contain the information how local regions in the data are connected to neighboring regions.\n",
    "\n",
    "The lower plot shows the low-d sigmoid function. The black lines connecting the plots of the high-d sigmoid and the low-d sigmoid indicate to which low-dimensional distances high-dimensional distences will ideally be mapped with your choice of sigmoid parameters.\n",
    "\n",
    "The sigmoid parameters for the low-d space can be selected according to the following rules:  \n",
    "`sig_l` = 1  (is irrelevant as it only scales the low-dimensional map)  \n",
    "`a_l` = a_h * n_dimensions_l / n_dimensions_h  \n",
    "`b_l`= b_h  \n",
    "Further information about the the selection of these sigmoid parameters can be found in the [Sketchmap literature](http://sketchmap.org).\n",
    "\n",
    "Feel free to play with different sigmoid parameters and see how the sigmoid function changes in the previous cell. I recommend to continue the tutorial with (0.3, 6, 6, 1, 4, 6) for a start but you can come back later and changes these parameters.\n",
    "\n",
    "In the next cell, you can set the sigmoid parameters and save then in the `parameters` instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Setting the parameters { run: \"auto\", vertical-output: true }\n",
    "\n",
    "sig_h = 0.3 # @param {type:\"number\"}\n",
    "a_h = 6 # @param {type:\"number\"}\n",
    "b_h = 6 # @param {type:\"number\"}\n",
    "sig_l = 1 # @param {type:\"number\"}\n",
    "a_l = 4 # @param {type:\"number\"}\n",
    "b_l = 6 # @param {type:\"number\"}\n",
    "parameters.dist_sig_parameters = (sig_h, a_h, b_h, sig_l, a_l, b_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "E9oMtBIvEWgI",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Get more info about parameters\n",
    "\n",
    "To get more information from your parameters use the `.parameters` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(parameters.parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "LKkL-0Q7EWgI",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<a id='perform-dimensionality-reduction'></a>\n",
    "\n",
    "## Perform Dimensionality Reduction\n",
    "\n",
    "Now that we have set up the parameters and loaded the data, it is very simple to performe the dimensionality reduction. All we need to do is to create an EncoderMap object and call its `train` method. The EncoderMap object takes care of setting up the neural network autoencoder and once you call the `train` method this network is trained to minimize the cost function as specified in the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_map = em.EncoderMap(parameters, high_d_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = e_map.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rpg1mmLPEWgI"
   },
   "source": [
    "Once the network is trained we can feed high dimensional data into the encoder part of the network and read the values from the bottleneck layer. That is how we project data to the low dimensional space. The following line projects all our high-dimensional data to the low-dimensional space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_d_projection = e_map.encode(high_d_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "kPUu9-XwEWgI",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Let's have a look at the result and plot the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(x=low_d_projection[:, 0], y=low_d_projection[:, 1], color=df[\"id\"].values, color_continuous_scale  = plotly.colors.sequential.Viridis)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "yHMmBnpOEWgJ",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<a id='generate-high-dimensional-data'></a>\n",
    "\n",
    "## Generate High-Dimensional Data\n",
    "\n",
    "We can not only use the encoder part of the network to project points the to the low-dimensional space. Also, the inverse procedure is possible using the decoder part of the Network. This allows to project any point from the low-dimensional space to the high dimensional space.  \n",
    "In the following we feed all low-dimension points into the decoder part of the network to generate high dimensional points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated = e_map.generate(low_d_projection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "kPhLyGUuEWgJ",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Let's have a look at these generated point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_3d(x=generated[:, 0], y=generated[:, 1], z=generated[:, 2], color=df[\"id\"].values, color_continuous_scale  = plotly.colors.sequential.Viridis)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "FO0D33HSEWgJ",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "You probable see again a cube like structure. The reconstruction, however, will not be perfect, as information is lost when the data is projected to a lower dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BYsEytiwEWgJ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial you have learned:\n",
    "- How to set parameters of EncoderMap\n",
    "- Instantiate an `EncoderMap` class with these parameters.\n",
    "- Run the dimensionality reduction\n",
    "- project points from the high-dimensional space to the low dimensional space and vice versa."
   ]
  }
 ],
 "metadata": {
  "emap": "run",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
