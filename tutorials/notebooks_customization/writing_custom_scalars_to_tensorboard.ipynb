{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68feb89b",
   "metadata": {},
   "source": [
    "# Logging Custom Scalars\n",
    "\n",
    "Run this notebook on Google Colab:\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/AG-Peter/encodermap/blob/main/tutorials/notebooks_customization/writing_custom_scalars_to_tensorboard.ipynb)\n",
    "\n",
    "Find the documentation of EncoderMap:\n",
    "\n",
    "https://ag-peter.github.io/encodermap\n",
    "\n",
    "### For Google colab only:\n",
    "\n",
    "If you're on Google colab, please uncomment these lines and install EncoderMap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f28dfe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-01T12:49:00.160170Z",
     "start_time": "2023-02-01T12:49:00.156662Z"
    }
   },
   "outputs": [],
   "source": [
    "# !wget https://raw.githubusercontent.com/AG-Peter/encodermap/main/tutorials/install_encodermap_google_colab.sh\n",
    "# !sudo bash install_encodermap_google_colab.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1e4269",
   "metadata": {},
   "source": [
    "## Primer\n",
    "\n",
    "In this tutorial we will explore 2 different ways of logging custom scalars or custom data to tensorboard using EncoderMap and its codebase. First let us start with the required imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3913fd7c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-01T12:49:04.587819Z",
     "start_time": "2023-02-01T12:49:03.769981Z"
    }
   },
   "outputs": [],
   "source": [
    "import encodermap as em\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99013363",
   "metadata": {},
   "source": [
    "## Subclassing a model and adding tf.summary\n",
    "\n",
    "**Idea:** Log different auto_cost_variants to tensorboard and see how they compare.\n",
    "\n",
    "EncoderMap's `auto_cost()` function compares the input and output of the autoencoder. The greater the difference the higher the returned loss. The next code fragment is taken from EncoderMap's code.\n",
    "\n",
    "```python\n",
    "def auto_cost_function(y_true, y_pred=None):\n",
    "    if y_pred is None:\n",
    "        y_pred = model(y_true)\n",
    "        \n",
    "    if p.auto_cost_scale is not None:\n",
    "        if p.auto_cost_variant == \"mean_square\":\n",
    "            auto_cost = tf.reduce_mean(\n",
    "                tf.square(periodic_distance(y_true, y_pred, p.periodicity)))\n",
    "        elif p.auto_cost_variant == \"mean_abs\":\n",
    "            auto_cost = tf.reduce_mean(\n",
    "                tf.abs(periodic_distance(y_true, y_pred, p.periodicity)))\n",
    "        elif p.auto_cost_variant == \"mean_norm\":\n",
    "            auto_cost = tf.reduce_mean(\n",
    "                tf.norm(periodic_distance(y_true, y_pred, p.periodicity), axis=1))\n",
    "        else:\n",
    "            raise ValueError(\"auto_cost_variant {} not available\".format(p.auto_cost_variant))\n",
    "        if p.auto_cost_scale != 0:\n",
    "            auto_cost *= p.auto_cost_scale\n",
    "    else:\n",
    "        auto_cost = 0\n",
    "    tf.cond(write_bool, true_fn=lambda: _summary_cost('Auto Cost', auto_cost),\n",
    "            false_fn=lambda: _do_nothing(), name=\"Cost\")\n",
    "```\n",
    "\n",
    "This loss takes the model input (`y_true`) and the model output (`y_pred`ict). If `y_pred` is not provided it will be created by calling the model `y_pred = model(y_true)`. Then, there are three ways of calculating the loss. They all differ in the way the mean is calculated. We have:\n",
    "\n",
    "- mean_square\n",
    "- mean_abs\n",
    "- mean_norm\n",
    "\n",
    "However, all of them are called on the output of a function called `periodic_distance()`. This function calculates the pairwise distances of all points, while recognizing periodicity. Pairwise distances of two sets of $n$ points yield a $n \\times n$ matrix. The dimensionality of the points does not matter. Distance between two points in $\\mathbb{R}^\\mathbb{N}$ space is just a scalar. Some data can lie in a periodic space (or a hypertoroidal manifold, if you're a mathematician). If the distances are greater than the provided periodicity they are wrapped around in periodic space. This means if you provide angles as y_true, they will be in the interval [$-\\pi$, $\\pi$] and the distance between $-\\frac{\\pi}{4}$ and $\\frac{\\pi}{4}$ is not $\\frac{3\\pi}{2}$, but rather $\\frac{\\pi}{2}$, because the space wraps around with periodicty $2\\pi$.\n",
    "\n",
    "**Only ever one of the different means is used as a loss. The other ones are never even calculated.**\n",
    "\n",
    "We will now use the `auto_cost()` mean_abs variant to train our NN model, but we will log all three losses to tensorboard. First of all we will use the SequentialModel provided by EncoderMap. This sequential model will create a simple Autoencoder network from the specifications in EncoderMap's Parameter class and the input. We will use the dihedral data from `asp7.csv` for this task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0598644",
   "metadata": {},
   "source": [
    "### Getting input data\n",
    "\n",
    "We'll use pandas to read the .csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cdad8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('asp7.csv')\n",
    "dihedrals = df.iloc[:,:-1].values.astype(np.float32)\n",
    "cluster_ids = df.iloc[:,-1].values\n",
    "print(dihedrals.shape, cluster_ids.shape)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e057ee",
   "metadata": {},
   "source": [
    "### Setting parameters\n",
    "\n",
    "Because we will use dihedrals mapped onto the range [-pi, pi], we will use a periodicity of 2\\*pi. Also: Don't forget to turn tensorboard True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f764e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = em.Parameters(\n",
    "tensorboard=True,\n",
    "periodicity=2*np.pi,\n",
    "n_steps=100,\n",
    "main_path=em.misc.run_path('runs/custom_scalars')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27be623c",
   "metadata": {},
   "source": [
    "### Subclassing the SequentialModel\n",
    "\n",
    "We create a new class inheriting form EncoderMap's `SequentialModel` and call it `MyModel`. We don't even need an `__init__()` method. Everything will be kept the same, we will just change stuff around in the method `train_step()`.\n",
    "\n",
    "The `SequentialModel` class wants two inpts: The input-shape and the parameters which will be used to deal with periodicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e3e811",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(em.models.models.SequentialModel):\n",
    "    pass\n",
    "\n",
    "my_model = MyModel(dihedrals.shape[1], parameters)\n",
    "print(my_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc4b2a5",
   "metadata": {},
   "source": [
    "Due to class inheritance the `MyModel` class can access the provided parameters as an instance variable called `p`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825b9db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(my_model.p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fecb6d",
   "metadata": {},
   "source": [
    "### Changing what happens in a training step\n",
    "\n",
    "Now we ill change what happens in a training step. We will simply call the parent's class `train_step()` function and add our custom code. Our custom code will be added inbetween the two lines reading:\n",
    "\n",
    "```python\n",
    "parent_class_out = super().train_step(data)\n",
    "return parent_class_out\n",
    "```\n",
    "\n",
    "The `train_step()` method takes besides the usual `self` instance, an argument called data. That is a batched input to the model. After every training step, a new batch will be randomly selected and shuffled from the input dataset to ensure the model reaches a good degree of generalization. We will use this input and call the model on that to get the model's output: `self(data)`. The input and output can now be compared similarly to the `auto_loss()` function. We still need one piece to do this. We will import the `periodic_distance()` function from encodermap and use it as is.\n",
    "\n",
    "After these values have been calculated we can write them to tensorboard using the `tf.summary.scalar()` function. We will group them all into a common namespace called `Comparison_Auto_Cost`.\n",
    "\n",
    "The last thing we need to talk about: The usage of `data[0]`. This is because Tensorflow generally assumes a classification task, where data[0] is the train data and data[1] is the train labels. Because we are doing a regression task, we will not use the second part of data. The `train_step()` method of the parent class also does something similar:\n",
    "\n",
    "\n",
    "```python\n",
    "def train_step(self, data):\n",
    "    \"\"\"Overwrites the normal train_step. What is different?\n",
    "\n",
    "    Not much. Even the provided data is expected to be a tuple of (data, classes) (x, y) in classification tasks.\n",
    "    The data is unpacked and y is discarded, because the Autoencoder Model is a regression task.\n",
    "\n",
    "    Args:\n",
    "        data (tuple): The (x, y) data of this train step.\n",
    "\n",
    "    \"\"\"\n",
    "    x, _ = data\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118e1272",
   "metadata": {},
   "outputs": [],
   "source": [
    "from encodermap.misc.distances import periodic_distance\n",
    "\n",
    "class MyModel(em.models.models.SequentialModel):\n",
    "    def train_step(self, data):\n",
    "        parent_class_out = super().train_step(data)\n",
    "        \n",
    "        # call the model on input\n",
    "        out = self.call(data[0])\n",
    "        \n",
    "        # calculate periodic distance with instance variable self.p containing parameters\n",
    "        p_dists = periodic_distance(data[0], out, self.p.periodicity)\n",
    "        \n",
    "        # use the different norms\n",
    "        mean_square = tf.reduce_mean(tf.square(p_dists))\n",
    "        mean_abs = tf.reduce_mean(tf.abs(p_dists))\n",
    "        mean_norm = tf.reduce_mean(tf.norm(p_dists, axis=1))\n",
    "        \n",
    "        # write the values to tensorboard\n",
    "        with tf.name_scope('Comparison_Auto_Cost'):\n",
    "            tf.summary.scalar('Mean Square', mean_square)\n",
    "            tf.summary.scalar('Mean Abs', mean_abs)\n",
    "            tf.summary.scalar('Mean Norm', mean_norm)\n",
    "        \n",
    "        # return the output of the parent's class train_step() function.\n",
    "        return parent_class_out\n",
    "    \n",
    "my_model = MyModel(dihedrals.shape[1], parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f13906",
   "metadata": {},
   "source": [
    "### Running EncoderMap with the new model\n",
    "\n",
    "How do we train the model? We provide an instance of our custom model to EncoderMap's `EncoderMap` class and let it handle the rest for us.\n",
    "\n",
    "Also make sure to execute tensorboard in the correct directory:\n",
    "\n",
    "```bash\n",
    "$ tensorboard --logdir . --reload_multifile True\n",
    "```\n",
    "\n",
    "If you're on Google colab, you can use tensorboard, by activating the tensorboard extension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b5677a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-01T12:58:49.243141Z",
     "start_time": "2023-02-01T12:58:49.239484Z"
    }
   },
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcc7465",
   "metadata": {},
   "outputs": [],
   "source": [
    "e_map = em.EncoderMap(parameters, dihedrals, model=my_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a85543",
   "metadata": {},
   "outputs": [],
   "source": [
    "e_map.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c705931",
   "metadata": {},
   "source": [
    "Here's what Tensorboard should put out:\n",
    "\n",
    "<img src=\"custom_scalars_1.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69aab19e",
   "metadata": {},
   "source": [
    "## Second way: Writing custom metrics.\n",
    "\n",
    "A metric that is used to judge how well your model performs. In contrast to losses metrics are not actively minimized during training. Metrics often involve more complex calculations and are not carried out for every training step.\n",
    "\n",
    "**Let us write an RMSD-mertric that computes the RMSD between the input and output of the AngleDihedralCartesianEncoderMap.**\n",
    "\n",
    "The RMSD between a set of coordinates is defined as:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f42fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mdtraj as md\n",
    "help(md.rmsd)"
   ]
  }
 ],
 "metadata": {
  "author": "mes",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
