{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2be1906-f144-410e-b65e-35538dded4f7",
   "metadata": {},
   "source": [
    "# Learning Rate Schedulers\n",
    "\n",
    "**Welcome**\n",
    "\n",
    "Welcome to the Learning Rate Schedulers tutorial. Learning rate schedulers can help us dynamically adjust the learning rate of the Adam optimization algorithm. That way, we can decrease the learning rate as we approach the minima of the cost function.\n",
    "\n",
    "Run this notebook on Google Colab:\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/AG-Peter/encodermap/blob/main/tutorials/notebooks_customization/04_learning_rate_schedulers.ipynb)\n",
    "\n",
    "Find the documentation of EncoderMap:\n",
    "\n",
    "https://ag-peter.github.io/encodermap\n",
    "\n",
    "**Goals:**\n",
    "\n",
    "In this tutorial you will learn:\n",
    "\n",
    "* [Why we can profit from learning rate schedulers](#why-learning-rate-schedulers?-a-linear-regression-exam-le)\n",
    "* [How to log the current learning rate to TensorBoard](#log-the-current-learning-rate-to-tensorboard)\n",
    "* [How to implement a learning rate scheduler with an exponentially decaying learning rate](#write-a-learning-rate-scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65568a51-1a34-4727-938b-955e135f94ce",
   "metadata": {},
   "source": [
    "### For Google colab only:\n",
    "\n",
    "If you're on Google colab, please uncomment these lines and install EncoderMap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408f06e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://gist.githubusercontent.com/kevinsawade/deda578a3c6f26640ae905a3557e4ed1/raw/b7403a37710cb881839186da96d4d117e50abf36/install_encodermap_google_colab.sh\n",
    "# !sudo bash install_encodermap_google_colab.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016b78d1",
   "metadata": {},
   "source": [
    "If you're on Google Colab, you also want to download the data we will use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16654191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://raw.githubusercontent.com/AG-Peter/encodermap/main/tutorials/notebooks_starter/asp7.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43decefb",
   "metadata": {},
   "source": [
    "## Import Libraries\n",
    "\n",
    "Before we can start exploring the learning rate scheduler, we need to import some libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6be3e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import encodermap as em\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05def998",
   "metadata": {},
   "source": [
    "We wil work in the directory `runs/lr_scheduler`. We will create it now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234a1751-d9f5-4e8c-8a7f-cef281c2117c",
   "metadata": {},
   "outputs": [],
   "source": [
    "(Path.cwd() / \"runs/lr_scheduler\").mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674b6f11-95d6-4634-ba56-be87a5fb6677",
   "metadata": {},
   "source": [
    "<a id=\"why-learning-rate-schedulers?-a-linear-regression-exam-le\"></a>\n",
    "\n",
    "## Why learning rate schedulers? A linear regression example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809d7c7e-f9ed-4b79-beda-3293c96263c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c1063547",
   "metadata": {},
   "source": [
    "<a id=\"log-the-current-learning-rate-to-tensorboard\"></a>\n",
    "\n",
    "## Log the current learning rate to Tensorboard\n",
    "\n",
    "Before we implement some dynamic learning rates we want to find a way to log the learning rate to tensorboard.\n",
    "\n",
    "### Running tensorboard on Google colab\n",
    "\n",
    "To use tensorboard in google colabs notebooks, you neet to first load the tensorboard extension\n",
    "\n",
    "```python\n",
    "%load_ext tensorboard\n",
    "```\n",
    "\n",
    "And then activate it with:\n",
    "\n",
    "```python\n",
    "%tensorboard --logdir .\n",
    "```\n",
    "\n",
    "The next code cell contains these commands. Uncomment them and then continue.\n",
    "\n",
    "### Running tensorboard locally\n",
    "\n",
    "TensorBoard is a visualization tool from the machine learning library TensorFlow which is used by the EncoderMap package. During the dimensionality reduction step, when the neural network autoencoder is trained, several readings are saved in a TensorBoard format. All output files are saved to the path defined in `parameters.main_path`. Navigate to this location in a shell and start TensorBoard. Change the paramter Tensorboard to `True` to make Encodermap log to Tensorboard.\n",
    "\n",
    "In case you run this tutorial in the provided Docker container you can open a new console inside the container by typing the following command in a new system shell.\n",
    "```shell\n",
    "docker exec -it emap bash\n",
    "```\n",
    "Navigate to the location where all the runs are saved. e.g.:\n",
    "```shell\n",
    "cd notebooks_easy/runs/asp7/\n",
    "```\n",
    "Start TensorBoard in this directory with:\n",
    "```shell\n",
    "tensorboard --logdir .\n",
    "```\n",
    "\n",
    "You should now be able to open TensorBoard in your webbrowser on port 6006.  \n",
    "`0.0.0.0:6006` or `127.0.0.1:6006`\n",
    "\n",
    "In the SCALARS tab of TensorBoard you should see among other values the overall cost and different contributions to the cost. The two most important contributions are `auto_cost` and `distance_cost`. `auto_cost` indicates differences between the inputs and outputs of the autoencoder. `distance_cost` is the part of the cost function which compares pairwise distances in the input space and the low-dimensional (latent) space.\n",
    "\n",
    "**Fixing Reloading issues**\n",
    "Using Tensorboard we often encountered some issues while training multiple models and writing mutliple runs to Tensorboard's logdir. Reloading the data and event refreshing the web page did not display the data of the current run. We needed to kill tensorboard and restart it in order to see the new data. This issue was fixed by setting `reload_multifile` `True`.\n",
    "\n",
    "```bash\n",
    "tensorboard --logdir . --reload_multifile True\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd7f3c8-2502-4c18-a40e-78cbf138909d",
   "metadata": {},
   "source": [
    "**When you're on Goole Colab, you can load the Tensorboard extension with:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5bcabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe6f9ac-3f9f-4a63-b3da-eeb95ae97602",
   "metadata": {},
   "source": [
    "### Sublcassing EncoderMap's `EncoderMapBaseCallback`\n",
    "\n",
    "The easiest way to implement and log a new variable to TensorBorard is by subclassing EncoderMap's `EncodeMapBaseCallback` from the `callbacks` submodule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e188df5c-47a3-4bca-86d8-cb8911d31fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "?em.callbacks.EncoderMapBaseCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563c9b43",
   "metadata": {},
   "source": [
    "As per the docstring of the `EncoderMapBaseCallback` class, we create the `LearningRateLogger` class and implement a piece of code in the `on_summary_step` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c34f6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningRateLogger(em.callbacks.EncoderMapBaseCallback):\n",
    "    def on_summary_step(self, step, logs=None):\n",
    "        with tf.name_scope(\"Learning Rate\"):\n",
    "            tf.summary.scalar('current learning rate', self.model.optimizer.lr, step=step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f53d462",
   "metadata": {},
   "source": [
    "We can now create an `EncoderMap` class and add our new callback with the `add_callback` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf99bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('asp7.csv')\n",
    "dihedrals = df.iloc[:,:-1].values.astype(np.float32)\n",
    "cluster_ids = df.iloc[:,-1].values\n",
    "\n",
    "parameters = em.Parameters(\n",
    "tensorboard=True,\n",
    "periodicity=2*np.pi,\n",
    "main_path=em.misc.run_path('runs/lr_scheduler'),\n",
    "n_steps=100,\n",
    "summary_step=5\n",
    ")\n",
    "\n",
    "# create an instance of EncoderMap\n",
    "e_map = em.EncoderMap(parameters, dihedrals)\n",
    "\n",
    "# Add an instance of the new Callback\n",
    "e_map.add_callback(LearningRateLogger)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e214e7f1",
   "metadata": {},
   "source": [
    "We train the Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96569684",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = e_map.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5688138",
   "metadata": {},
   "source": [
    "And now, we can see our current leanring rate in TensorBoard\n",
    "\n",
    "<img src=\"lr_scheduler_1.png\" width=\"800\">\n",
    "\n",
    "A constant learning rate of 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598068c0",
   "metadata": {},
   "source": [
    "<a id=\"write-a-learning-rate-scheduler\"></a>\n",
    "\n",
    "## Write a learning rate scheduler\n",
    "\n",
    "We can write a learning rate scheduler either by providing intervals of training steps and the associated learning rate:\n",
    "\n",
    "```python\n",
    "def lr_schedule(step):\n",
    "    \"\"\"\n",
    "    Returns a custom learning rate that decreases as steps progress.\n",
    "    \"\"\"\n",
    "    learning_rate = 0.2\n",
    "    if step > 10:\n",
    "        learning_rate = 0.02\n",
    "    if step > 20:\n",
    "        learning_rate = 0.01\n",
    "    if step > 50:\n",
    "        learning_rate = 0.005\n",
    "```\n",
    "\n",
    "Or by using a function that gives us a learning rate:\n",
    "\n",
    "```python\n",
    "def scheduler(step, lr=1, n_steps=1000):\n",
    "    \"\"\"\n",
    "    Returns a custom learning rate that decreases based on an exp function as steps progress.\n",
    "    \"\"\"\n",
    "    if step < 10:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * tf.math.exp(-step / n_steps)\n",
    "```\n",
    "\n",
    "Below, is an example combining both:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a37f62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(step, lr=1):\n",
    "    \"\"\"\n",
    "    Returns a custom learning rate that decreases based on an exp function as steps progress.\n",
    "    \"\"\"\n",
    "    if step < 10:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * tf.math.exp(-0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a848aa1d",
   "metadata": {},
   "source": [
    "This scheduler function can simply be provided to the builtin `keras.callbacks.LearningRateScheduler` callback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332faaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = tf.keras.callbacks.LearningRateScheduler(scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f22dc5",
   "metadata": {},
   "source": [
    "And appended to the list of `callbacks` in the EncoderMap class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9087259e",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = em.Parameters(\n",
    "tensorboard=True,\n",
    "periodicity=2*np.pi,\n",
    "main_path=em.misc.run_path('runs/lr_scheduler'),\n",
    "n_steps=50,\n",
    "summary_step=1\n",
    ")\n",
    "\n",
    "e_map = em.EncoderMap(parameters, dihedrals)\n",
    "e_map.add_callback(LearningRateLogger)\n",
    "e_map.add_callback(callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b026be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = e_map.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7973533f",
   "metadata": {},
   "source": [
    "Here's what Tensorboard should look like:\n",
    "\n",
    "<img src=\"lr_scheduler_2.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2372c499-3c60-420c-9e3f-c0f67a08e268",
   "metadata": {},
   "source": [
    "And here's the learning rate plotted from the history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669a5b1b-4ea2-4652-a039-0ce23381ef4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "px.line(history.history[\"lr\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fc18f6",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Learning rate schedulers are helpful to prevent overtraining, but still slightly increase the predictive power of your NN model. EncoderMap's modularity allows for them to be simple Plug-In solutions."
   ]
  }
 ],
 "metadata": {
  "emap": "run",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
