{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68feb89b",
   "metadata": {},
   "source": [
    "# Customize EncoderMap: Logging Custom Scalars\n",
    "\n",
    "**Welcome**\n",
    "\n",
    "Welcome to the customization part of the EncoderMap tutorials. EncoderMap was redesigned from the ground up using the great customizability of the TensorFlow library. In the new version of EncoderMap all objects can be changed, adjusted by the user or even reused in other TensorFlow projects. The notebooks in this section help you in customizing EnocderMap and adding custom functionality.\n",
    "\n",
    "This notebook specifically helps you in logging custom scalars to TensorBoard to visualize additional data during the training of EncoderMap's networks on your data and help you investigate the problems at hand.\n",
    "\n",
    "Run this notebook on Google Colab:\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/AG-Peter/encodermap/blob/main/tutorials/notebooks_customization/01_custom_scalar_to_tensorboard.ipynb)\n",
    "\n",
    "Find the documentation of EncoderMap:\n",
    "\n",
    "https://ag-peter.github.io/encodermap\n",
    "\n",
    "**Goals**\n",
    "\n",
    "In this tutorial you will learn:\n",
    "\n",
    "* [How to subclass EncoderMap's `EncoderMapBaseMetric` to add additonal logging capability to TensorBoard](#adding-custom-scalars-to-tensorboard-by-subclassing-encodermapbasemetric)\n",
    "* [Use the `y_true` and `y_pred` parmeters in the `update()` function](#use-the-y_true-and-y_pred-arguments-in-the-update()-function)\n",
    "* [Subclass EncoderMap's model and change `training_step()`](#subclassing-encodermap's-models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb952f1-c86f-47db-9d1a-40dbc22be56f",
   "metadata": {},
   "source": [
    "### For Google colab only\n",
    "\n",
    "If you're on Google colab, please uncomment these lines and install EncoderMap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f28dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://gist.githubusercontent.com/kevinsawade/deda578a3c6f26640ae905a3557e4ed1/raw/b7403a37710cb881839186da96d4d117e50abf36/install_encodermap_google_colab.sh\n",
    "# !sudo bash install_encodermap_google_colab.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f8181d-0763-492a-a2dd-25f2aac4e26d",
   "metadata": {},
   "source": [
    "If you're on Google Colab, you also want to download the data we will use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c5066a-9785-4228-bb06-4fabf9ff375d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://raw.githubusercontent.com/AG-Peter/encodermap/main/tutorials/notebooks_starter/asp7.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1e4269",
   "metadata": {},
   "source": [
    "## Import libraries\n",
    "\n",
    "before we can start exploring how to add custom data to TensorBoard, we need to import some libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3913fd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import encodermap as em\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35f8b21-3992-415f-993c-f45094297ffd",
   "metadata": {},
   "source": [
    "<a id=\"adding-custom-scalars-to-tensorboard-by-subclassing-encodermapbasemetric\"></a>\n",
    "\n",
    "## Adding custom scalars to TensorBoard by subclassing EncoderMapBaseMetric\n",
    "\n",
    "EncoderMap has implemented a `EncoderMapBaseMetric` class, that can be used to implement such features. It can be found in the `callbacks` submodule in EncoderMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a44fdf-ed65-4b26-934a-026d2222d818",
   "metadata": {},
   "outputs": [],
   "source": [
    "?em.callbacks.EncoderMapBaseMetric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5836407e-99dd-4745-8416-14dcf31a0351",
   "metadata": {},
   "source": [
    "We will subclass `EncoderMapBaseMetric` to add additional logging capabilities to our training. As a first example, we will just log a random-normal value. For that we create our own `Metric` class. We only need to implement a single method, called `update`. Normally this method gets the input of the network as the `y_true` argument and the output as the `y_pred` argument (remember. EncoderMap is a regression task and so the `y_true` values do not stem from training data, but are the input data, that the network tries to regress against). However, in our case we won't need these values, as we just take samples from a random normal distribution. Here, it is best to use the builtin tensorflow function `tf.random.normal()`, with the NumPy function `np.random.normal`, the random state will not be updated and the output will be constant (rather than random).\n",
    "\n",
    "To log the random value, we also need to use `tf.summary.scalar()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231e15b7-aaab-4800-a48e-c936755ae97a",
   "metadata": {},
   "source": [
    "### Create a custom Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e983ae-7033-4279-b1e0-c73dbec8b1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomNormalMetric(em.callbacks.EncoderMapBaseMetric):\n",
    "    def update(self, y_true, y_pred):\n",
    "        r = tf.random.normal(shape=(1, ))[0]\n",
    "        tf.summary.scalar(\"my random metric\", r)\n",
    "        return r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d21495-6b35-47d8-82b2-24c7fcad5806",
   "metadata": {},
   "source": [
    "This metric can easily be added to a `EncoderMap` instance via the `add_metric()` method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ecc1fd-95e8-4457-a19e-cc79ee54e3f5",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856f71fa-9b8b-4524-b601-eef4964cf42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = em.Parameters(n_steps=1_000, tensorboard=True)\n",
    "emap = em.EncoderMap(parameters=p)\n",
    "emap.add_metric(RandomNormalMetric)\n",
    "history = emap.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd3792b-aaca-4bf4-bea0-a9910b47d8d3",
   "metadata": {},
   "source": [
    "Our custom metric will be available in the `'RandomNormalMetric Metric'` key of the history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd89d082-b1e6-49a9-b5cb-c7b668879702",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(\n",
    "    data=[\n",
    "        go.Histogram(x=history.history[\"RandomNormalMetric Metric\"], nbinsx=20)\n",
    "    ]\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1966ed-8ddf-42c4-ba23-99feac2d06de",
   "metadata": {},
   "source": [
    "In tensorboard, the custom scalar can be found in the Scalars section:\n",
    "\n",
    "<img src=\"custom_scalars_1.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96e51ad-5100-46b1-8abf-3c8bab92a835",
   "metadata": {},
   "source": [
    "<a id=\"use-the-y_true-and-y_pred-arguments-in-the-update()-function\"></a>\n",
    "\n",
    "## Use the y_true and y_pred arguments in the update() function\n",
    "\n",
    "To get a feel how these parameters can be used, when subclassing an `EncoderMapBaseMetric`, we will have a look at one of EncoderMap's cost functions. The *auto cost* compares the input and output pairwise distances. In EncoderMap, there are three variants of doing so:\n",
    "\n",
    "1) `mean_square`:\n",
    "The `mean_square` variant is computed via:\n",
    "```python\n",
    "auto_cost = tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "```\n",
    "2) `mean_abs`:\n",
    "The `mean_abs` variant is computed via:\n",
    "```python\n",
    "auto_cost = tf.reduce_mean(tf.abs(y_true - y_pred))\n",
    "```\n",
    "3) `mean_norm`:\n",
    "The `mean_norm` variant is computed via:\n",
    "```python\n",
    "auto_cost = tf.reduce_mean(tf.norm(y_true - y_pred))\n",
    "```\n",
    "\n",
    "However, during training only one of these variants will be emplyed. Let's write some metrics, that will calculate the cost variants regardless of which variant is actually used during training. For that, we will create three new `em.callbacks.EncoderMapBaseMetric` subclasses:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114241cc-0001-4ab4-83b7-a6d160ba5596",
   "metadata": {},
   "source": [
    "### Define the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b1641c-951d-4b40-919b-da5f4b4c3cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanSquare(em.callbacks.EncoderMapBaseMetric):\n",
    "    def update(self, y_true, y_pred):\n",
    "        c = tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "        tf.summary.scalar(\"mean square\", c)\n",
    "        return c\n",
    "\n",
    "\n",
    "class MeanAbs(em.callbacks.EncoderMapBaseMetric):\n",
    "    def update(self, y_true, y_pred):\n",
    "        c = tf.reduce_mean(tf.abs(y_true - y_pred))\n",
    "        tf.summary.scalar(\"mean abs\", c)\n",
    "        return c\n",
    "\n",
    "\n",
    "class MeanNorm(em.callbacks.EncoderMapBaseMetric):\n",
    "    def update(self, y_true, y_pred):\n",
    "        c = tf.reduce_mean(tf.norm(y_true - y_pred))\n",
    "        tf.summary.scalar(\"mean norm\", c)\n",
    "        return c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a00337-488b-4973-ab29-72f61b7dbc50",
   "metadata": {},
   "source": [
    "We will also add a new metric which logs the maximum value of the y_true value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec23b73f-402d-4790-b3b6-c6b81a44f651",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxVal(em.callbacks.EncoderMapBaseMetric):\n",
    "    def update(self, y_true, y_pred):\n",
    "        c = tf.reduce_max(y_true)\n",
    "        tf.summary.scalar(\"mean norm\", c)\n",
    "        return c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf51c325-2dde-4b80-89c6-7e89b780b822",
   "metadata": {},
   "source": [
    "With these new metrics, we can train another instance of the `EncoderMap` network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbc0012-b05c-4064-8aa4-66267adc7164",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dfde07-5603-4e26-ada0-ed866b2a4941",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = em.Parameters(n_steps=1_000, tensorboard=True)\n",
    "emap = em.EncoderMap(parameters=p)\n",
    "emap.add_metric(MeanSquare)\n",
    "emap.add_metric(MeanAbs)\n",
    "emap.add_metric(MeanNorm)\n",
    "emap.add_metric(MaxVal)\n",
    "history = emap.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd15e96-7234-4011-8518-d3a9af11f9f8",
   "metadata": {},
   "source": [
    "And have a look at how these metrics compare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931b8eec-6912-4d77-bb89-4ffb55946368",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "\n",
    "fig = make_subplots(rows=2, cols=1)\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        y=history.history[\"MeanSquare Metric\"],\n",
    "        mode=\"lines\",\n",
    "        name=\"mean square\",\n",
    "    ),\n",
    "    col=1,\n",
    "    row=1,\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        y=history.history[\"MeanAbs Metric\"],\n",
    "        mode=\"lines\",\n",
    "        name=\"mean abs\",\n",
    "    ),\n",
    "    col=1,\n",
    "    row=1,\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        y=history.history[\"MeanNorm Metric\"],\n",
    "        mode=\"lines\",\n",
    "        name=\"mean norm\",\n",
    "    ),\n",
    "    col=1,\n",
    "    row=1,\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        y=history.history[\"MaxVal Metric\"],\n",
    "        mode=\"lines\",\n",
    "        name=\"maximum value of y_true\",\n",
    "    ),\n",
    "    col=1,\n",
    "    row=2,\n",
    ")\n",
    "fig.update_layout(width=1000, height=500)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea44e2e1-2ce8-421f-9710-e742fc2836e1",
   "metadata": {},
   "source": [
    "<a id=\"subclassing-encodermap's-models\"></a>\n",
    "## Subclassing EncoderMap's models\n",
    "\n",
    "The last method of logging custom scalars we will look at generates a custom TensorFlow model, that contains additional code that logs to TensorBoard. Let us start by collecting the Input data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0598644",
   "metadata": {},
   "source": [
    "### Getting input data\n",
    "\n",
    "We'll use pandas to read the .csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cdad8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('asp7.csv')\n",
    "dihedrals = df.iloc[:,:-1].values.astype(np.float32)\n",
    "cluster_ids = df.iloc[:,-1].values\n",
    "print(dihedrals.shape, cluster_ids.shape)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e057ee",
   "metadata": {},
   "source": [
    "### Setting parameters\n",
    "\n",
    "Because we will use dihedrals mapped onto the range [-pi, pi], we will use a periodicity of 2\\*pi. Also: Don't forget to turn tensorboard True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f764e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = em.Parameters(\n",
    "tensorboard=True,\n",
    "periodicity=2*np.pi,\n",
    "n_steps=100,\n",
    "main_path=em.misc.run_path('runs/custom_scalars')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27be623c",
   "metadata": {},
   "source": [
    "### Subclassing the SequentialModel\n",
    "\n",
    "We create a new class inheriting form EncoderMap's `SequentialModel` and call it `MyModel`. We don't even need an `__init__()` method. Everything will be kept the same, we will just change stuff around in the method `train_step()`.\n",
    "\n",
    "The `SequentialModel` class wants two inpts: The input-shape and the parameters which will be used to deal with periodicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e3e811",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(em.models.models.SequentialModel):\n",
    "    pass\n",
    "\n",
    "my_model = MyModel(dihedrals.shape[1], parameters)\n",
    "print(my_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc4b2a5",
   "metadata": {},
   "source": [
    "Due to class inheritance the `MyModel` class can access the provided parameters as an instance variable called `p`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825b9db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(my_model.p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fecb6d",
   "metadata": {},
   "source": [
    "### Changing what happens in a training step\n",
    "\n",
    "Now we ill change what happens in a training step. We will simply call the parent's class `train_step()` function and add our custom code. Our custom code will be added inbetween the two lines reading:\n",
    "\n",
    "```python\n",
    "parent_class_out = super().train_step(data)\n",
    "return parent_class_out\n",
    "```\n",
    "\n",
    "The `train_step()` method takes besides the usual `self` instance, an argument called data. That is a batched input to the model. After every training step, a new batch will be randomly selected and shuffled from the input dataset to ensure the model reaches a good degree of generalization. We will use this input and call the model on that to get the model's output: `self(data)`. The input and output can now be compared similarly to the `auto_loss()` function. We still need one piece to do this. We will import the `periodic_distance()` function from encodermap and use it as is.\n",
    "\n",
    "After these values have been calculated we can write them to tensorboard using the `tf.summary.scalar()` function. We will group them all into a common namespace called `Comparison_Auto_Cost`.\n",
    "\n",
    "The last thing we need to talk about: The usage of `data[0]`. This is because Tensorflow generally assumes a classification task, where data[0] is the train data and data[1] is the train labels. Because we are doing a regression task, we will not use the second part of data. The `train_step()` method of the parent class also does something similar:\n",
    "\n",
    "\n",
    "```python\n",
    "def train_step(self, data):\n",
    "    \"\"\"Overwrites the normal train_step. What is different?\n",
    "\n",
    "    Not much. Even the provided data is expected to be a tuple of (data, classes) (x, y) in classification tasks.\n",
    "    The data is unpacked and y is discarded, because the Autoencoder Model is a regression task.\n",
    "\n",
    "    Args:\n",
    "        data (tuple): The (x, y) data of this train step.\n",
    "\n",
    "    \"\"\"\n",
    "    x, _ = data\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118e1272",
   "metadata": {},
   "outputs": [],
   "source": [
    "from encodermap.misc.distances import periodic_distance\n",
    "\n",
    "class MyModel(em.models.models.SequentialModel):\n",
    "    def train_step(self, data):\n",
    "        parent_class_out = super().train_step(data)\n",
    "        \n",
    "        # call the model on input\n",
    "        out = self.call(data[0])\n",
    "        \n",
    "        # calculate periodic distance with instance variable self.p containing parameters\n",
    "        p_dists = periodic_distance(data[0], out, self.p.periodicity)\n",
    "        \n",
    "        # use the different norms\n",
    "        mean_square = tf.reduce_mean(tf.square(p_dists))\n",
    "        mean_abs = tf.reduce_mean(tf.abs(p_dists))\n",
    "        mean_norm = tf.reduce_mean(tf.norm(p_dists, axis=1))\n",
    "        \n",
    "        # write the values to tensorboard\n",
    "        with tf.name_scope('Comparison_Auto_Cost'):\n",
    "            tf.summary.scalar('Mean Square', mean_square)\n",
    "            tf.summary.scalar('Mean Abs', mean_abs)\n",
    "            tf.summary.scalar('Mean Norm', mean_norm)\n",
    "        \n",
    "        # return the output of the parent's class train_step() function.\n",
    "        return parent_class_out\n",
    "    \n",
    "my_model = MyModel(dihedrals.shape[1], parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f13906",
   "metadata": {},
   "source": [
    "### Running EncoderMap with the new model\n",
    "\n",
    "How do we train the model? We provide an instance of our custom model to EncoderMap's `EncoderMap` class and let it handle the rest for us.\n",
    "\n",
    "Also make sure to execute tensorboard in the correct directory:\n",
    "\n",
    "```bash\n",
    "$ tensorboard --logdir . --reload_multifile True\n",
    "```\n",
    "\n",
    "If you're on Google colab, you can use tensorboard, by activating the tensorboard extension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b5677a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcc7465",
   "metadata": {},
   "outputs": [],
   "source": [
    "e_map = em.EncoderMap(parameters, dihedrals, model=my_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a85543",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = e_map.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849bf65f-2871-4638-9476-b242b60683ee",
   "metadata": {},
   "source": [
    "Here's what Tensorboard should put out:\n",
    "\n",
    "<img src=\"custom_scalars_2.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d43fda-9aa3-457c-8434-acfab8d44aa3",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial you have learned three ways of logging custom scalars to TensorBoard with EncoderMap. You have learned about Metrices, how to subclass EncoderMap's `EncoderMapBaseMetric`, and add it to running trainings. You have learned about subclassing EncoderMap's `SequentialModel`."
   ]
  }
 ],
 "metadata": {
  "emap": "run",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
