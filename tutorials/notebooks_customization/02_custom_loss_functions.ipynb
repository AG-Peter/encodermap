{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16da5ed1-982b-40d7-ba84-8b39c3cf5e0c",
   "metadata": {},
   "source": [
    "# Customize EncoderMap: Custom loss functions\n",
    "\n",
    "**Welcome**\n",
    "\n",
    "Welcome to the second part of the customization section.\n",
    "\n",
    "Run this notebook on Google Colab:\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/AG-Peter/encodermap/blob/main/tutorials/notebooks_customization/02_custom_loss_functions.ipynb)\n",
    "\n",
    "Find the documentation of EncoderMap:\n",
    "\n",
    "https://ag-peter.github.io/encodermap\n",
    "\n",
    "**Goals:**\n",
    "\n",
    "In this tuorial you will learn:\n",
    "- [A few general concepts about loss functions.](#what-are-loss-functions)\n",
    "- [What loss functions are implemented in EncoderMap.](#cost-functions)\n",
    "- [How you can write your own loss functions.](#custom-cost-functions)\n",
    "- [Another example for a custom loss function.](#adding-a-unit-circle-loss)\n",
    "\n",
    "**For Google colab only**\n",
    "\n",
    "If you're on Google colab, please uncomment these lines and install EncoderMap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ff2eeb-f982-4f3b-9bfc-dc9574d197d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://gist.githubusercontent.com/kevinsawade/deda578a3c6f26640ae905a3557e4ed1/raw/b7403a37710cb881839186da96d4d117e50abf36/install_encodermap_google_colab.sh\n",
    "# !sudo bash install_encodermap_google_colab.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9da4dcf-3e0e-40ce-a114-578ee8a9054a",
   "metadata": {},
   "source": [
    "## Import Libraries\n",
    "\n",
    "In this tutorial we will learn how to write our own loss functions and add them to EncoderMap. Let us start with the imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4eaab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.subplots as ps\n",
    "import tensorflow as tf\n",
    "\n",
    "import encodermap as em\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a938cf02-38b9-4f94-a629-1141d8f54f7a",
   "metadata": {},
   "source": [
    "<a id='what-are-loss-functions'></a>\n",
    "\n",
    "## What are loss functions\n",
    "\n",
    "Loss functions in EncoderMap are small pieces of code, that take some inputs and return scalar values. We can easily come up with a loss function ourselves, when we think about a linear regression. First let us create some example data in a $\\mathbb{R}^2$ space. Each point $i$ is defined by an x-value $x^{(i)}$ and a true y-value $y^{(i)}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5306dd-2062-4ac4-8a85-cca1f291cbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 10, 200)\n",
    "y = np.linspace(0, 10, 200)\n",
    "\n",
    "# add noise\n",
    "y += (np.random.random((200, )) - 0.5) * (np.abs(y - 5) + 1)\n",
    "\n",
    "fig = px.scatter(x=x, y=y, labels={\"x\": \"x\", \"y\": \"y\"})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcebf5d4-9952-429d-abf9-d328c808ae66",
   "metadata": {},
   "source": [
    "The model describing our data will be a linear function without a y-intercept. This model takes in an actual x-value of the data $x$, multiplies it with a value $w$ and produces a prediction $\\hat{y}$.\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{y} = wx\n",
    "\\end{equation}\n",
    "\n",
    "And the cost function $J(w)$ is defined as the squared distance between a sample $y$ and our prediction $\\hat{y}$. Our cost function is a function of the parameter $w$:\n",
    "\n",
    "\\begin{align}\n",
    "J(w) &= \\frac{1}{2m} \\sum_{i=1}^m \\left( \\hat{y}^{(i)} - y^{(i)} \\right) ^ 2\\\\\n",
    "&= \\frac{1}{2m} \\sum_{i=1}^m \\left( wx^{(i)} - y^{(i)} \\right) ^ 2\n",
    "\\end{align}\n",
    "\n",
    "We can plot this loss function using a range of $w$ values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f253ad2-169d-42ed-a03b-530995f8b2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def y_hat(x, w):\n",
    "    return w * x\n",
    "\n",
    "def J(w):\n",
    "    loss = 0\n",
    "    for x_sample, y_true in zip(x, y):\n",
    "        y_pred = y_hat(x_sample, w)\n",
    "        loss += (y_pred - y_true) ** 2\n",
    "    loss /= (2 * len(x))\n",
    "    return loss\n",
    "\n",
    "test_w = np.linspace(0, 2, 20)\n",
    "test_J = [J(w) for w in test_w]\n",
    "\n",
    "fig = px.line(x=test_w, y=test_J, labels={\"x\": \"w\", \"y\": \"J\"})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c719044-5206-47e6-8d15-8480d72826c4",
   "metadata": {},
   "source": [
    "Doing so, we can get a good idea, what $w$ might produce the best fit for our data. However, to find the minimum algorithmically, we need to write a gradient descent algorithm. For that we need the derivative $\\frac{d}{dw}$ of our cost function $J(w)$, i.e. the gradient.\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{d}{dw} J(w) = \\frac{1}{m} \\sum_{i=1}^m \\left( \\hat{y}^{(i)} - y^{(i)} \\right)x^{(i)}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ca7640-5201-4d26-ac87-23da4452e9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(epochs, alpha=0.001, J_conv=0.01):\n",
    "    w_test = np.random.random((1, ))[0] * 2\n",
    "    losses = []\n",
    "    for i in range(epochs):\n",
    "        loss_derivative = 0\n",
    "        for x_sample, y_sample in zip(x, y):\n",
    "            loss_derivative += (y_hat(x_sample, w_test) - y_sample) * x_sample\n",
    "        loss_derivative /= len(x)\n",
    "        w_test -= alpha * loss_derivative\n",
    "        loss = J(w_test)\n",
    "        losses.append((i, \"w\", w_test))\n",
    "        losses.append((i, \"J\", loss))\n",
    "    return pd.DataFrame(np.array(losses), columns=[\"epoch\", \"type\", \"value\"]).astype({\"epoch\": int, \"type\": str, \"value\": float})\n",
    "\n",
    "\n",
    "history = gradient_descent(200)\n",
    "fig = px.line(history, x=\"epoch\", y=\"value\", color=\"type\")\n",
    "fig.update_traces(mode=\"lines\", hovertemplate=\"%{y:.4f}\")\n",
    "fig.update_layout(hovermode=\"x\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f56341f-85b2-4cf6-9771-4183a52df563",
   "metadata": {},
   "source": [
    "We can now use this value to plot our regression line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b1e4b8-50b0-433e-9f78-b7c1a00fc0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = history[history[\"type\"] == \"w\"][\"value\"].values[-1]\n",
    "y_fit = y_hat(x, w)\n",
    "fig = go.Figure(\n",
    "    data=[\n",
    "        go.Scatter(x=x, y=y, mode=\"markers\", name=\"data\"),\n",
    "        go.Scatter(x=x, y=y_fit, mode=\"lines\", name=\"fit\"),\n",
    "    ]\n",
    ")\n",
    "fig.update_layout(\n",
    "    {\n",
    "        \"xaxis\": {\n",
    "            \"title\": \"x\"\n",
    "        },\n",
    "        \"yaxis\": {\n",
    "            \"title\": \"y\"\n",
    "        },\n",
    "    }\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a065831-b53c-4e60-8bfe-667ef46c9d3e",
   "metadata": {},
   "source": [
    "Adding confidence intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a091f86-aedb-4302-9b49-e0e0b148bb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_confidence(y_hat, y, y_fit, ci=0.95):\n",
    "    # standard deviation of y_hat = y_bar\n",
    "    sum_errs = sum((y - y_fit) ** 2)\n",
    "    y_bar = np.sqrt(1 / (len(y_fit) - 2) * sum_errs)\n",
    "\n",
    "    # interval for standard normal distribution\n",
    "    # ci = 1 - ci\n",
    "    ppf = 1 - (ci / 2)\n",
    "    z_score_lookup = (ppf - 0) / 0.5\n",
    "\n",
    "    interval = z_score_lookup * y_bar\n",
    "\n",
    "    lower = y_hat - interval\n",
    "    upper = y_hat + interval\n",
    "    return lower, upper\n",
    "\n",
    "lower = []\n",
    "upper = []\n",
    "for i in y_fit:\n",
    "    l, u = get_confidence(i, y, y_fit)\n",
    "    lower.append(l)\n",
    "    upper.append(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6046c873-13f0-4211-830f-53255d250c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_fit = y_hat(x, w)\n",
    "fig = go.Figure(\n",
    "    data=[\n",
    "        go.Scatter(x=x, y=y, mode=\"markers\", name=\"data\"),\n",
    "        go.Scatter(x=x, y=y_fit, mode=\"lines\", name=\"fit\"),\n",
    "        go.Scatter(x=np.concatenate([x, x[::-1]]), y=np.concatenate([lower, upper[::-1]]), mode=\"lines\", name=\"confidence\", fill=\"toself\", line_width=0),\n",
    "    ]\n",
    ")\n",
    "fig.update_layout(\n",
    "    {\n",
    "        \"xaxis\": {\n",
    "            \"title\": \"x\"\n",
    "        },\n",
    "        \"yaxis\": {\n",
    "            \"title\": \"y\"\n",
    "        },\n",
    "    }\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ec18cb-881d-4f49-bb4c-1a8d81fa39f4",
   "metadata": {},
   "source": [
    "<a id='cost-functions'></a>\n",
    "\n",
    "## Cost functions\n",
    "\n",
    "### Cost functions in TensorFlow\n",
    "\n",
    "Cost functions in TensorFlow can easily be implemented via simple functions, that take the actual and predicted values of a neural network model, do custom arithmetic and return a scalar value. The cost function of the linear regression in TensorFlow can be written as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73122d2d-a488-41b8-95f4-223d9af6682a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def J_tf(y_true, y_pred):\n",
    "    return tf.reduce_mean((y_true - y_pred) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e66f0aa-dac0-4040-becd-0ec70e60a64f",
   "metadata": {},
   "source": [
    "This function can now without any alterations be used as the cost function for our model. To predict the slope of our data we have can use a TensorFlow model with a single neuron. That neuron's weight will be adjusted by the optimizer until the loss function is minimal. This should give us a similar result than what our custom gradient descent implementation has produced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1ee23d-10ff-42e7-bac2-0e4b882dcaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(units=1, activation=\"linear\", input_shape=(1, ))\n",
    "])\n",
    "\n",
    "model.compile(loss=J_tf)\n",
    "\n",
    "model.fit(x, y, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175042e9-3861-454c-b2d5-d54d02d6565c",
   "metadata": {},
   "source": [
    "We will now use the `predict()` method of the trained model to construct our linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e77ee3-f111-4fa5-bbe6-538455106450",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_fit = model.predict(x)[:, 0]\n",
    "fig = go.Figure(\n",
    "    data=[\n",
    "        go.Scatter(x=x, y=y, mode=\"markers\", name=\"data\"),\n",
    "        go.Scatter(x=x, y=y_fit, mode=\"lines\", name=\"fit\"),\n",
    "    ]\n",
    ")\n",
    "fig.update_layout(\n",
    "    {\n",
    "        \"xaxis\": {\n",
    "            \"title\": \"x\"\n",
    "        },\n",
    "        \"yaxis\": {\n",
    "            \"title\": \"y\"\n",
    "        },\n",
    "    }\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715b120c-3329-45dc-b599-7ae3fc7d7a6c",
   "metadata": {},
   "source": [
    "### Cost functions in EncoderMap\n",
    "\n",
    "The model EncoderMap employs is a bit more complicated than the single neuron linear regression model we've just created. The main difference lies in EncoderMap calculating costs from intermediate layers. The multi-dimensional scaling like cost functions for example use the output of the latent (bottleneck) layer and the input of the network to calculate a scalar loss. That's why EncoderMap's cost functions are almost always closures. Meaning they return a function and not a value. This is done, so that the inner functions have access to the TensorFlow model and can access intermediate layers. Let's look at an example closure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df8a9ef-1f66-46f6-b51b-6ab5bab41a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def outer(argument):\n",
    "    print(\"Setting up inner function\")\n",
    "    def inner(different_argument):\n",
    "        print(\n",
    "            f\"The inner function has access to the \"\n",
    "            f\"Outer namespace {argument=} but also \"\n",
    "            f\"to its own namespace {different_argument=}\"\n",
    "        )\n",
    "    return inner\n",
    "\n",
    "func = outer(\"Hello!\")\n",
    "func2 = outer(\"Foo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815c0ec9-3309-4d19-b811-4e79c2a8f34e",
   "metadata": {},
   "source": [
    "The function returned by the closure can now be called normally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a887abc5-3af7-4d9c-8b1c-02c805ba5ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "func(\"World!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f764a2f9-8ed3-4601-9bad-31925674fdb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "func2(\"Bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4f3528-afaa-4e80-8804-57d017cad24a",
   "metadata": {},
   "source": [
    "#### Autoencoder cost\n",
    "\n",
    "Let us first take a look at the *auto_cost*. This function can be summarized as follows: \n",
    "\n",
    "- Take the input of the autoencoder network as $y$\n",
    "- Take the output of the autoencoder network as $\\hat{y}$\n",
    "- Build the difference $d = y - \\hat{y}$\n",
    "- Make this difference positive by using one of these functions:\n",
    "  - absolute $l = \\lvert d \\rvert$\n",
    "  - square $l = d^2$\n",
    "  - norm $l = \\lVert d \\rVert$\n",
    "- Take the mean of all samples as the scalar cost value.\n",
    "\n",
    "This loss function can be implemented like so:\n",
    "\n",
    "```python\n",
    "def auto_loss(model, parameters=None):\n",
    "    if parameters is None:\n",
    "        p = Parameters()\n",
    "    else:\n",
    "        p = parameters\n",
    "    def auto_loss_func(y_true, y_pred=None):\n",
    "        if y_pred is None:\n",
    "            y_pred = model(y_true)\n",
    "        if p.auto_cost_scale is not None:\n",
    "            if p.auto_cost_variant == \"mean_square\":\n",
    "                auto_cost = tf.reduce_mean(\n",
    "                    tf.square(periodic_distance(y_true, y_pred, p.periodicity))\n",
    "                )\n",
    "            elif p.auto_cost_variant == \"mean_abs\":\n",
    "                auto_cost = tf.reduce_mean(\n",
    "                    tf.abs(periodic_distance(y_true, y_pred, p.periodicity))\n",
    "                )\n",
    "            elif p.auto_cost_variant == \"mean_norm\":\n",
    "                auto_cost = tf.reduce_mean(\n",
    "                    tf.norm(periodic_distance(y_true, y_pred, p.periodicity), axis=1)\n",
    "                )\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    \"auto_cost_variant {} not available\".format(p.auto_cost_variant)\n",
    "                )\n",
    "            if p.auto_cost_scale != 0:\n",
    "                auto_cost *= p.auto_cost_scale\n",
    "        else:\n",
    "            auto_cost = 0.0\n",
    "        return auto_cost\n",
    "\n",
    "    return auto_loss_func\n",
    "        \n",
    "```\n",
    "\n",
    "Let's look at this function in a bit more detail. The outer function of the close (`auto_loss()`) takes a TensorFlow model and a instance of `em.Parameters` as its inputs. The `model` argument is used in the inner function (`auto_loss_func()`) to get the `y_pred` value regardless of whether it was provided to the inner function. The `p` variable from the outer function is used in the inner function to define a periodicity (`p.periodicity`) for input data that lie in a periodic space (angles). Furthermore it is used to define a `p.auto_cost_scale` and a `p.auto_cost_variant`. These options can be set by the user in the parameter class and are finally brough to use here. Let's have a look at another cost function in EncoderMap employing the output of intermediate layers.\n",
    "\n",
    "#### Distance loss\n",
    "\n",
    "The so-called *distance loss* compares the output of the neurons in the latent space with the network input. Thus, it needs access to these intermediate neurons, which is - again - done via a closue.\n",
    "\n",
    "```python\n",
    "def distance_loss(model, parameters=None):\n",
    "    if parameters is None:\n",
    "        p = Parameters()\n",
    "    else:\n",
    "        p = parameters\n",
    "\n",
    "    latent = model.encoder\n",
    "\n",
    "    # we will come back to this later\n",
    "    dist_loss = sigmoid_loss(p)\n",
    "\n",
    "    def distance_loss_func(y_true, y_pred):\n",
    "        y_pred = latent(y_true)\n",
    "        \n",
    "        # functional model gives a tuple\n",
    "        if isinstance(y_true, tuple):\n",
    "            y_true = tf.concat(y_true[:3], axis=1)\n",
    "        if p.distance_cost_scale is not None:\n",
    "            dist_cost = dist_loss(y_true, y_pred)\n",
    "            if p.distance_cost_scale != 0:\n",
    "                dist_cost *= p.distance_cost_scale\n",
    "        else:\n",
    "            dist_cost = 0.0\n",
    "        return dist_cost\n",
    "    return distance_loss_func\n",
    "```\n",
    "\n",
    "With the information from the autoencoder loss, we can easily understand this loss function. In the outer function, the same handling of the `parameters` argument is done. Inside the inner function this object's `p.distance_cost_scale` attribute is used. Furthermore, the encoder sub-model is extracted from the `model` argument via the line `latent = model.encoder` and the encoder output (here called `y_pred`) is generated from calling this sub-model (`latent(y_true)`) in the inner function. The last line of the outer function (`dist_loss = sigmoid_loss(p)`) uses another closure which you can see below.\n",
    "\n",
    "```python\n",
    "def sigmoid_loss(parameters):\n",
    "    if parameters is None:\n",
    "        p = Parameters()\n",
    "    else:\n",
    "        p = parameters\n",
    "    periodicity = p.periodicity\n",
    "    dist_sig_parameters = p.dist_sig_parameters\n",
    "    def sigmoid_loss_func(y_true: tf.Tensor, y_pred: tf.Tensor) -> tf.Tensor:\n",
    "        r_h = y_true\n",
    "        r_l = y_pred\n",
    "        if periodicity == float(\"inf\"):\n",
    "            dist_h = pairwise_dist(r_h)\n",
    "        else:\n",
    "            dist_h = pairwise_dist_periodic(r_h, periodicity)\n",
    "        dist_l = pairwise_dist(r_l)\n",
    "\n",
    "        sig_h = sigmoid(*dist_sig_parameters[:3])(dist_h)\n",
    "        sig_l = sigmoid(*dist_sig_parameters[3:])(dist_l)\n",
    "\n",
    "        cost = tf.reduce_mean(tf.square(sig_h - sig_l))\n",
    "        return cost\n",
    "\n",
    "    return sigmoid_loss_func\n",
    "```\n",
    "\n",
    "This function implements the sigmoid scaled multi-dimensional scaling described in the original sketch-map paper [[1]](#cite-sketchmap). The `periodicity` and the user-specified `dist_sig_parameters` are used inside the inner function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f773cad-582a-45d7-89cf-bfa0ff4e6231",
   "metadata": {},
   "source": [
    "<a id='custom-cost-functions'></a>\n",
    "\n",
    "## Custom cost functions\n",
    "\n",
    "We can now try to write custom loss functions and also compare how they affect the training of the EncoderMap network. We will compare the EncoderMap network \n",
    "\n",
    "1) No cost function, that compares the lowd and the input. A regular AutoEncoder.\n",
    "2) A MDS-like cost function that compares the pairwise distances in low and high-dimensional space.\n",
    "3) The Sketch-map like cost function that weighs those costs using a high and low-dimensional sigmoid.\n",
    "\n",
    "### FAT10\n",
    "\n",
    "First, let us load some example data taken from published research [[2]](#cite-fat10). Please cite the original authors if you're using this dataset for your work.\n",
    "\n",
    "We can load the data using EncoderMap's `load_project()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f6923e-b7f5-4ec6-8959-7bd773905c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_file = Path(\"/home/kevin/encodermap/tests/data/FAT10/trajs.h5\")\n",
    "if not traj_file.is_file():\n",
    "    trajs = em.load(\n",
    "        trajs=list(Path(\"/home/kevin/encodermap/tests/data/FAT10\").glob(\"*.xtc\")),\n",
    "        tops=list(Path(\"/home/kevin/encodermap/tests/data/FAT10\").glob(\"*.pdb\")),\n",
    "    )\n",
    "    trajs.load_CVs(\"all\")\n",
    "    trajs.save(\"/home/kevin/encodermap/tests/data/FAT10/trajs.h5\")\n",
    "else:\n",
    "    trajs = em.load(traj_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e8ef90-3490-4056-b6dd-59b637bb66a4",
   "metadata": {},
   "source": [
    "From the CV data of the `TrajEnsemble`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68dea04d-9e1b-4c8e-b7b0-a893863c56b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trajs._CVs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca464d4-710a-48e9-a2da-8c6d15e9a34d",
   "metadata": {},
   "source": [
    "We will only use the central (backbone) dihedrals. Because the dataset is also very large we also will use just a subset of datapoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ba4355-4fab-4b3a-bdb9-60724af540e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "trajs._CVs.central_dihedrals.sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c193be-da89-4466-b2a1-e65d0004f243",
   "metadata": {},
   "source": [
    "This can be done using some xarray functions without loading the large dataset into memory (RAM). We will stack the xarray functions `.stack()` to combine the `'traj_num'` and `'frame_num'` axes into one combined `'frame'` axis, the `transpose()` function to  make the new '`frame'` axis the first axis. We will also use the `dropna()` function to drop frames which contain only `np.nan` values. This can happen, when some trajectories in the `TrajEnsemble` have differenring numbers of frames. From this dataset, we will only use every hundredth point (`[::100]`) for the next image. Only when calling the `.value` property of the dataset is it loaded into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68be6050-dcfd-4115-b138-d6e2e492b4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dihedrals = (\n",
    "    trajs._CVs.central_dihedrals\n",
    "    .stack({\"frame\": (\"traj_num\", \"frame_num\")})\n",
    "    .transpose(\"frame\", ...)\n",
    "    .dropna(\"frame\", how=\"all\")\n",
    ")[::100].values\n",
    "dihedrals.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a6b71d-acc0-40e8-9f46-4b10b38c34de",
   "metadata": {},
   "source": [
    "We can now take a look at how these dihedrals would be mapped into the low-dimensional space after weighing them with the sigmoid function.\n",
    "If your're running the notebook on Google Colab, BinderHub, or locally on your PC you can play araound with the sliders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99edaabf-37ee-40d1-bd93-56dee997885b",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = em.Parameters()\n",
    "em.plot.distance_histogram_interactive(\n",
    "    data=dihedrals[::10],\n",
    "    periodicity=2*np.pi,\n",
    "    parameters=p,\n",
    "    initial_guess=[12.8, 7.4, 12, 2.3, 2.0, 6.0],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d9b0d6-4dbb-48b8-98e9-883591f8037d",
   "metadata": {},
   "source": [
    "To use all available datapoints as training input for EncoderMap, we can use the `tf_dataset()` method of the `TrajEnsemble` object. This method takes a `batch_size` and a `CV_name` argument, that we will set accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010afce6-2b50-48a3-8cc7-0893432d354b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fat10_dataset = trajs.tf_dataset(\n",
    "    batch_size=256,\n",
    "    CV_names=[\"central_dihedrals\"],\n",
    ")\n",
    "fat10_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f293dd-31ab-4e4b-b411-8903717a288a",
   "metadata": {},
   "source": [
    "#### Autoencoder\n",
    "\n",
    "We will now train `EncoderMap` as if it was a simple autoencoder. That is, without a MDS-like scaling function. This can be done by setting the `distance_cost_scale` parameter to zero. We will also set the `training` parameter to `'custom'` to tell EncoderMap, that the dataset we give it is already ready to use (otherwise EncoderMap wants to batch and repeat the dataset).\n",
    "\n",
    "The instantiation of parameters and the training will be done in one single cell. If you want to learn more about the `EncoderMap` (and `Autoencoder`) class, `Parameters` and training, you can refer to the starter notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb4280d-a245-49de-9567-b07a11ef5e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = em.Parameters(\n",
    "    tensorboard=True,\n",
    "    main_path=em.misc.run_path(Path.cwd() / \"vary_cost_functions_emap/FAT10/autoencoder\"),\n",
    "    n_steps=3_000,\n",
    "    summary_step=10,\n",
    "    distance_cost_scale=0,\n",
    "    periodicity=2 * np.pi,\n",
    ")\n",
    "autoencoder = em.EncoderMap(\n",
    "    parameters=p,\n",
    "    train_data=fat10_dataset,\n",
    ")\n",
    "autoencoder.add_images_to_tensorboard(data=dihedrals)\n",
    "history = autoencoder.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a40c17-82c5-4381-8688-f420ae93ca5a",
   "metadata": {},
   "source": [
    "This model can now be used to project the data into a 2D low-dimensional representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75de3a25-9f44-4b63-ac9b-039afb44da92",
   "metadata": {},
   "outputs": [],
   "source": [
    "lowd = autoencoder.encode(trajs.central_dihedrals)\n",
    "em.plot.plot_free_energy(*lowd.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95545aea-e753-4416-a23a-449bb56715d2",
   "metadata": {},
   "source": [
    "#### EncoderMap\n",
    "\n",
    "By using the `EncoderMap` network, we can directly compare a regular autoencoder to EncoderMap. In this trainign, we will set the `distance_cost_scale` to its original value of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf5a998-0e39-4a7c-9aeb-1da5c70b806d",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = em.Parameters(\n",
    "    tensorboard=True,\n",
    "    main_path=em.misc.run_path(Path.cwd() / \"vary_cost_functions_emap/FAT10/encodermap\"),\n",
    "    n_steps=3_000,\n",
    "    summary_step=10,\n",
    "    distance_cost_scale=1,\n",
    "    periodicity=2 * np.pi,\n",
    ")\n",
    "encodermap = em.EncoderMap(\n",
    "    parameters=p,\n",
    "    train_data=fat10_dataset,\n",
    ")\n",
    "encodermap.add_images_to_tensorboard(data=dihedrals)\n",
    "history = encodermap.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa32539-fa3b-4318-bed4-06835000f717",
   "metadata": {},
   "source": [
    "We can now have a look at the low-dimensional projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69f2127-2d33-493d-93ae-8eb61cfec65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lowd = encodermap.encode(trajs.central_dihedrals)\n",
    "em.plot.plot_free_energy(*lowd.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eece7fc-4620-49a0-b397-90d020f0a0a6",
   "metadata": {},
   "source": [
    "#### Replacing the `distance_loss` with a true MDS loss\n",
    "\n",
    "In the last step, we will replace the `distance_loss` with a true MDS loss. For that, we have to define a new loss function, that we can add to the `EncoderMap` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d6a06a-6d75-4af1-9f64-1abe2f82922c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will import the function pairwise_dist()\n",
    "# and pairwise_dist_periodic() \n",
    "# from EncoderMap's misc sub-package.\n",
    "from encodermap.misc.distances import pairwise_dist, pairwise_dist_periodic\n",
    "\n",
    "\n",
    "def MDS_loss(model, parameters):\n",
    "    \"\"\"Outer function of our MDS loss.\n",
    "\n",
    "    Similar to the other loss closures in\n",
    "    EncoderMap, the outer function takes\n",
    "    two arguments: model and parameters.\n",
    "\n",
    "    We can also implement additinal logic\n",
    "    to print in the outer function.\n",
    "\n",
    "    Args:\n",
    "        model (tf.keras.Model): A tensorflow model.\n",
    "            This model is required to have an attribute\n",
    "            called `encoder`, which will be used\n",
    "            to get the output of the bottleneck neurons.\n",
    "        parameters (em.parameters.AnyParameters): An\n",
    "            instance of EncoderMap's parameter classes.\n",
    "\n",
    "    Returns:\n",
    "        Callable[[tf.Tensor, tf.Tensor], tf.Tensor]:\n",
    "            The inner function, which tensorflow uses\n",
    "            to calculate the MDS loss.\n",
    "    \n",
    "    \"\"\"\n",
    "    # to get intermediate output for loss calculations\n",
    "    latent = model.encoder\n",
    "    periodicity = parameters.periodicity\n",
    "\n",
    "    print(\"Outer function of MDS loss called.\")\n",
    "\n",
    "    def MDS_loss_fn(y_true, y_pred=None):\n",
    "        \"\"\"Inner MDS loss function.\n",
    "\n",
    "        The inner function must return a scalar value.\n",
    "\n",
    "        Args:\n",
    "            y_true (tf.Tensor): The input.\n",
    "            y_pred (tf.Tensor): The output. This\n",
    "                argument will not be used. We are not\n",
    "                interested in the output of the decoder.\n",
    "\n",
    "        Returns:\n",
    "            tf.Tensor: The output scalar tensor.\n",
    "        \n",
    "        \"\"\"\n",
    "        # lowd and highd are defines like so\n",
    "        lowd = latent(y_true)\n",
    "        highd = y_true\n",
    "\n",
    "        # pairwise distances are calculated like so\n",
    "        # for the input, we have to respect the \n",
    "        # periodicity of the input space.\n",
    "        pairwise_lowd = pairwise_dist(lowd)\n",
    "        pairwise_highd = pairwise_dist_periodic(\n",
    "            positions=highd,\n",
    "            periodicity=periodicity,\n",
    "        )\n",
    "\n",
    "        # The mean squared differences of the\n",
    "        # pairwise distances is the MDS cost\n",
    "        cost = tf.reduce_mean(\n",
    "            tf.square(\n",
    "                pairwise_highd - pairwise_lowd\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # log to tensorboard\n",
    "        tf.summary.scalar('MDS Cost', cost)\n",
    "        return cost\n",
    "    return MDS_loss_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a707b84-d0f7-4bdb-b3cd-5a22ddb1091e",
   "metadata": {},
   "source": [
    "When instantiating this instance of the EncoderMap class, we will set `distance_cost_scale=1` again to turn off this cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74eec79-649b-4c9b-8ba1-e1ffb9f6b601",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = em.Parameters(\n",
    "    tensorboard=True,\n",
    "    main_path=em.misc.run_path(Path.cwd() / \"vary_cost_functions_emap/FAT10/custom_mds\"),\n",
    "    n_steps=3_000,\n",
    "    summary_step=10,\n",
    "    distance_cost_scale=0,\n",
    "    periodicity=2 * np.pi,\n",
    ")\n",
    "custom_mds = em.EncoderMap(\n",
    "    parameters=p,\n",
    "    train_data=fat10_dataset,\n",
    ")\n",
    "custom_mds.add_loss(MDS_loss)\n",
    "custom_mds.add_images_to_tensorboard(data=dihedrals)\n",
    "history = custom_mds.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc4eda7-a55c-4939-bc60-76df5e0fa1ba",
   "metadata": {},
   "source": [
    "Finally, we will have a look at the projection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1336baaa-84da-4a60-8d5d-3887e6dbb58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lowd = custom_mds.encode(trajs.central_dihedrals)\n",
    "em.plot.plot_free_energy(*lowd.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87dcbec",
   "metadata": {},
   "source": [
    "<a id=\"adding-a-unit-circle-loss\"></a>\n",
    "\n",
    "### Adding a unit circle loss\n",
    "\n",
    "In this last example, we will do something silly. We will replace EncoderMap's `center_cost` with a loss that tries to push the low-dimensional points into a unit circle. For a unit circle the following equation holds true:\n",
    "\n",
    "\\begin{align}\n",
    "x^2 + y^2 &= 1\\\\\n",
    "x^2 + y^2 - 1 &= 0\n",
    "\\end{align}\n",
    "\n",
    "Let us first plot a unit circle with matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab8ffd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "t = np.linspace(0,np.pi*2,100)\n",
    "\n",
    "px.line(x=np.cos(t) + 5, y=np.sin(t), width=500, height=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5b54b2",
   "metadata": {},
   "source": [
    "**How to put this information into a loss function?**\n",
    "\n",
    "We need to find a function that describes the distance between any (x, y)-coordinate to the unit circle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e74ac7-e488-4602-82e8-25e8d70587ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_to_unit_circle_2D(x, y):\n",
    "    return np.abs((np.square(x) + np.square(y)) - 1)\n",
    "\n",
    "xx = np.linspace(-2, 2, 250)\n",
    "yy = np.linspace(-2, 2, 250)\n",
    "grid = np.meshgrid(xx, yy)\n",
    "z = distance_to_unit_circle_2D(*grid)\n",
    "\n",
    "px.imshow(z, height=500, width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be15a9f-1320-4afc-8a52-171e22af4739",
   "metadata": {},
   "source": [
    "The loss function can be written with a closure like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9424eafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def circle_loss(model, parameters):\n",
    "    \"\"\"Circle loss outer function. Takes model and parameters. Parameters is only here for demonstration purpoes.\n",
    "    It is not actually needed in the closure.\n",
    "\n",
    "    \"\"\"\n",
    "    # use the models encoder part to create low-dimensional data\n",
    "    latent = model.encoder\n",
    "\n",
    "    def circle_loss_fn(y_true, y_pred=None):\n",
    "        \"\"\"Circle loss inner function. Takes y_true and y_pred. y_pred will not be used. y_true will be used to get\n",
    "        the latent space of the autoencoder.\n",
    "\n",
    "        \"\"\"\n",
    "        # get latent output\n",
    "        lowd = latent(y_true)\n",
    "\n",
    "        # get circle cost\n",
    "        circle_cost = tf.reduce_mean(tf.abs(tf.reduce_sum(tf.square(lowd), axis=0) - 1))\n",
    "\n",
    "        # bump up the cost to make it stronger than the other contributions\n",
    "        circle_cost *= 5\n",
    "\n",
    "        # write to tensorboard\n",
    "        tf.summary.scalar('Circle Cost', circle_cost)\n",
    "\n",
    "        # return circle cost\n",
    "        return circle_cost\n",
    "\n",
    "    # return inner function\n",
    "    return circle_loss_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35da01e7",
   "metadata": {},
   "source": [
    "### Include the loss function in EncoderMap\n",
    "\n",
    "**First:** Let us load the dihedral data from `../notebooks_easy` and define some Parameters. For the parameters we will set the `center_cost_scale` to be 0 as to not interfere with our new circle cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90dc660e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('asp7.csv')\n",
    "dihedrals = df.iloc[:,:-1].values.astype(np.float32)\n",
    "cluster_ids = df.iloc[:,-1].values\n",
    "print(dihedrals.shape, cluster_ids.shape)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf5d3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = em.Parameters(\n",
    "    tensorboard=True,\n",
    "    center_cost_scale=0,\n",
    "    n_steps=400,\n",
    "    periodicity=2*np.pi,\n",
    "    main_path=em.misc.run_path('runs/custom_losses')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481468d2",
   "metadata": {},
   "source": [
    "Now we can instaniate the `EncoderMap` class. For visualization purposes we will also make tensorboard write images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2300f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "e_map = em.EncoderMap(parameters, dihedrals)\n",
    "e_map.add_images_to_tensorboard(dihedrals, image_step=1, save_to_disk=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc2ac47",
   "metadata": {},
   "source": [
    "We can now tell the `EncoderMap` instance to add our new loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3722415a",
   "metadata": {},
   "outputs": [],
   "source": [
    "e_map.add_loss(circle_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce54855a",
   "metadata": {},
   "source": [
    "Now we add this loss to `EncoderMap`'s losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4c51ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(e_map.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6138c276",
   "metadata": {},
   "source": [
    "### Train\n",
    "\n",
    "Also make sure to execute tensorboard in the correct directory:\n",
    "\n",
    "```bash\n",
    "$ tensorboard --logdir . --reload_multifile True\n",
    "```\n",
    "\n",
    "If you're on Google colab, you can use tensorboard, by activating the tensorboard extension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed7bb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5b4e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = e_map.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad885b65",
   "metadata": {},
   "source": [
    "### Output\n",
    "\n",
    "Here's what Tensorboard should put out:\n",
    "\n",
    "<img src=\"custom_loss_functions_1.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cd1d4c-f4c6-42f5-8b6b-9624f4fbcded",
   "metadata": {},
   "source": [
    "## Loading logs into jupyter notebook\n",
    "\n",
    "Last but not least, we will have a look at how we could retrieve the images and logs from TensorBoard. This is not trivial, as the `tfevent` files, that TensorFlow writes are so-called protobuf files (for protocol buffers, a data serialization format developed by Google)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d8e16f-330d-42e3-946e-fc7afaab7dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls runs/custom_losses/run0/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ab333c-1d41-4d8a-88ba-e7275dbb90f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.summary.summary_iterator import summary_iterator\n",
    "from io import BytesIO\n",
    "from plotly.subplots import make_subplots\n",
    "import base64\n",
    "from pathlib import Path\n",
    "\n",
    "records_files = list(\n",
    "    Path(parameters.main_path).rglob(\"*tfevents*\")\n",
    ")\n",
    "\n",
    "prefix = \"data:image/png;base64,\"\n",
    "\n",
    "visited_tags = {}\n",
    "image_steps = [0, 68, 399]\n",
    "images_at_steps = {}\n",
    "\n",
    "for records_file in records_files:\n",
    "    visited_tags[records_file] = set()\n",
    "    for i, summary in enumerate(summary_iterator(str(records_file))):\n",
    "        for j, v in enumerate(summary.summary.value):\n",
    "            visited_tags[records_file].add(v.tag)\n",
    "            if v.tag == \"Latent Output/Latent Density\":\n",
    "                b = tf.make_ndarray(v.tensor)[-1]\n",
    "                stream = BytesIO(b)\n",
    "                base64_string = prefix + base64.b64encode(stream.getvalue()).decode(\"utf-8\")\n",
    "                images_at_steps[summary.step] = base64_string\n",
    "\n",
    "fig = make_subplots(cols=3, rows=1, subplot_titles=[\"step 0\", \"step 12\", \"step 67\"])\n",
    "fig.add_trace(\n",
    "    go.Image(source=images_at_steps[0]), col=1, row=1,\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Image(source=images_at_steps[12]), col=2, row=1,\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Image(source=images_at_steps[67]), col=3, row=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9899200",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Using the closure method, you can easily add new loss functions to EncoderMap."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715a00d1-9972-412f-b272-ac87ebf9f036",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "<a id=\"cite-sketchmap\"></a>\n",
    "\n",
    "```\n",
    "@article{ceriotti2011simplifying,\n",
    "  title={Simplifying the representation of complex free-energy landscapes using sketch-map},\n",
    "  author={Ceriotti, Michele and Tribello, Gareth A and Parrinello, Michele},\n",
    "  journal={Proceedings of the National Academy of Sciences},\n",
    "  volume={108},\n",
    "  number={32},\n",
    "  pages={13023--13028},\n",
    "  year={2011},\n",
    "  publisher={National Acad Sciences}\n",
    "}\n",
    "```\n",
    "\n",
    "<a id=\"cite-h1ub\"></a>\n",
    "\n",
    "```\n",
    "@article{sawade2023combining,\n",
    "  title={Combining molecular dynamics simulations and scoring method to computationally model ubiquitylated linker histones in chromatosomes},\n",
    "  author={Sawade, Kevin and Marx, Andreas and Peter, Christine and Kukharenko, Oleksandra},\n",
    "  journal={PLoS Computational Biology},\n",
    "  volume={19},\n",
    "  number={8},\n",
    "  pages={e1010531},\n",
    "  year={2023},\n",
    "  publisher={Public Library of Science San Francisco, CA USA}\n",
    "}\n",
    "```\n",
    "\n",
    "<a id=\"cite-fat10\"></a>\n",
    "```\n",
    "@article{franke2023visualizing,\n",
    "  title={Visualizing the residue interaction landscape of proteins by temporal network embedding},\n",
    "  author={Franke, Leon and Peter, Christine},\n",
    "  journal={Journal of Chemical Theory and Computation},\n",
    "  volume={19},\n",
    "  number={10},\n",
    "  pages={2985--2995},\n",
    "  year={2023},\n",
    "  publisher={ACS Publications}\n",
    "}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "emap": "run",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
