{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cube Example - Basic Usage\n",
    "In this tutorial you will learn the basic usage of EncoderMap with a little toy data set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries\n",
    "Before we can get started using EncoderMap we first need to import the EncoderMap library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import encodermap as em"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also need some aditional imports for plotting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "Next, we need to load our data. EncoderMap expects the input data to be a 2d array. Each line should contain one data point and the number of columns is the dimensionality of the data set. Here, you could load data from any source. In this tutorial, however, we will use a function to generate a toy data set. The function `random_on_cube_edges` distributes a given number of points randomly on the edges of a cube. We can also add some Gaussian noise by specifying a sigma value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_d_data, ids = em.misc.random_on_cube_edges(5000, sigma=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's look at the data we have just created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"high_d_data\\n\", high_d_data)\n",
    "print(\"ids\\n\", ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`high_d_data` is a 2d array where each row represents one point in a 3d space. `ids` contains a number form 0 to 11 for each point. This number tells us which edge of the cube each point belongs to. We can use these `ids` to color the points in a plot. The following code creates a scatter plot of the data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "fig = plt.figure()\n",
    "axe = fig.add_subplot(111, projection='3d')\n",
    "axe.scatter(high_d_data[:, 0], high_d_data[:, 1], high_d_data[:, 2], \n",
    "            c=ids, marker=\"o\", linewidths=0, cmap=\"tab10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Parameters\n",
    "Now  that we have loaded our data we need to select parameters for EncoderMap. Parameters are stored in an object derived from the `Parameters` class. A list of the available parameters can be found [here](https://ag-peter.github.io/encodermap/docbuild/html/parameters.html#module-encodermap.parameters). We do not need to bother with all these parameters, as the default parameters should be fine in many cases. We should, however adjust some parameters for this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = em.Parameters()\n",
    "parameters.main_path = em.misc.run_path(\"runs/cube\")\n",
    "parameters.periodicity = float(\"inf\")\n",
    "parameters.n_steps = 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `main_path` defines where output files will be written. `run_path` is a helper function which creates a new numbered run directory. When you call this for the first time it will create a directory `runs/cube/run0`. If this directory already exist it will instead create the directory `runs/cube/run1` and so forth.\n",
    "\n",
    "The `periodicity` is important for periodic input such as angular values. As the input in this case in not periodic we set the periodicity to infinite.\n",
    "\n",
    "`n_steps` defines the number of training iterations.\n",
    "\n",
    "Furthermore, we should adjust the sigmoid functions applied to the high-dimensional and low-dimensional pairwise distances of the distance based part of the cost function. There a three parameters for each sigmoid which should be given in the following order:  \n",
    "(sig_h, a_h, b_h, sig_l, a_l, b_l)  \n",
    "In order to select these parameters it is helpful to plot the sigmoid functions together with a histogram of the pairwise distances in the data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters.dist_sig_parameters = (0.3, 6, 6, 1, 4, 6)\n",
    "em.plot.distance_histogram(high_d_data, \n",
    "                           parameters.periodicity, \n",
    "                           parameters.dist_sig_parameters,\n",
    "                           bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The upper plot shows the high-d sigmoid function and the pairwise distances histogram, the plot also shows the derivative of the sigmoid function. This derivative shows the sensitive range of the distance based part of the cost function. As it is not possible to preserve all pairwise distances in the low-d representation we want to tune this sensitive range to match distances which are most important for us. Usually very short distances are not important for the structure of a data set as these distances stem from points inside the same local region. Long distances might be interesting but can hardly be reproduced in a lower dimensional representation. Somewhere in between are the most important distances which contain the information how local regions in the data are connected to neighboring regions.\n",
    "\n",
    "The lower plot shows the low-d sigmoid function. The black lines connecting the plots of the high-d sigmoid and the low-d sigmoid indicate to which low-dimensional distances high-dimensional distences will ideally be mapped with your choice of sigmoid parameters.\n",
    "\n",
    "The sigmoid parameters for the low-d space can be selected according to the following rules:  \n",
    "`sig_l` = 1  (is irrelevant as it only scales the low-dimensional map)  \n",
    "`a_l` = a_h * n_dimensions_l / n_dimensions_h  \n",
    "`b_l`= b_h  \n",
    "Further information about the the selection of these sigmoid parameters can be found in the [Sketchmap literature](http://sketchmap.org). \n",
    "\n",
    "Feel free to play with different sigmoid parameters and see how the sigmoid function changes in the previous cell. I recommend to continue the tutorial with (0.3, 6, 6, 1, 4, 6) for a start but you can come back later and changes these parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performe Dimensionality Reduction\n",
    "\n",
    "Now that we have set up the parameters and loaded the data, it is very simple to performe the dimensionality reduction. All we need to do is to create an EncoderMap object and call its `train` method. The EncoderMap object takes care of setting up the neural network autoencoder and once you call the `train` method this network is trained to minimize the cost function as specified in the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_map = em.EncoderMap(parameters, high_d_data)\n",
    "e_map.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the network is trained we can feed high dimensional data into the encoder part of the network and read the values from the bottleneck layer. That is how we project data to the low dimensional space. The following line projects all our high-dimensional data to the low-dimensional space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_d_projection = e_map.encode(high_d_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the result and plot the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axe = plt.subplots()\n",
    "axe.scatter(low_d_projection[:, 0], low_d_projection[:, 1], c=ids, s=5, marker=\"o\", linewidths=0, cmap=\"tab10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate High-Dimensional Data\n",
    "\n",
    "We can not only use the encoder part of the network to project points the to the low-dimensional space. Also, the inverse procedure is possible using the decoder part of the Network. This allows to project any point from the low-dimensional space to the high dimensional space.  \n",
    "In the following we feed all low-dimension points into the decoder part of the network to generate high dimensional points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated = e_map.generate(low_d_projection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at these generated point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "axe = fig.add_subplot(111, projection='3d')\n",
    "axe.scatter(generated[:, 0], generated[:, 1], generated[:, 2], c=ids, marker=\"o\", linewidths=0, cmap=\"tab10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You probable see again a cube like structure. The reconstruction, however, will not be perfect, as information is lost when the data is projected to a lower dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "In this tutorial you have learned how to set parameters, run the dimensionality reduction and how to project points from the high-dimensional space to the low dimensional space and vice versa."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
